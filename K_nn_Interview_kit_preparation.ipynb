{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K-nn Interview kit preparation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMAEIbbikUk9"
      },
      "source": [
        "# Explain about K-Nearest Neighbour?\n",
        "> Agorithm- \n",
        "*   In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point\n",
        "*   For continous variable , Euclidean Distanc is used\n",
        "*   For discrete variable(text classification) , Hamming Distance is used\n",
        "*   The classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis.\n",
        "*   A more frequent class tend to dominate the prediction of the new example, because they tend to be common among the k nearest neighbors due to their large number.\n",
        "*   One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its k nearest neighbors.\n",
        "*   The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point\n",
        "*   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqz8VeOfm96A"
      },
      "source": [
        "# Failure Cases of K-NN? \n",
        "1.  For an outlier: Since weight is inversely proportional to distance . For an outlier , any cluster with maximum weightage gives a nearest neighbiut value , which is an error\n",
        "2.  For large space and time complexity issue may result is failure of k-nn algorithm . this can be otimized by chossing suitable algorithm "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBMbE7_QoJha"
      },
      "source": [
        "# Euclidean Distance- \n",
        "*  The Euclidean distance between two points in Euclidean space is the length of a line segment between the two points\n",
        "*   The Normal distance that we find in co orodinate system \n",
        "# Manhattan Distance \n",
        "*   The distance between two points measured along axes at right angles. In a plane with p1 at (x1, y1) and p2 at (x2, y2), it is |x1 - x2| + |y1 - y2|.\n",
        "\n",
        "# Minkowski distance\n",
        "*  The Minkowski distance or Minkowski metric is a metric in a normed vector space which can be considered as a generalization of both the Euclidean distance and the Manhattan distance.  \n",
        "*  D(X,Y)=(sum|xi-yi|^p)^{1/p}\n",
        "*  At p=1 , it becomes Manhattan \n",
        "*  AT p=2 , it becomes Euclidean distance\n",
        "\n",
        "# Hamming Distance\n",
        "* Boolean Value \n",
        "* Used for image processing \n",
        "* Works good for 2D and 3D not more than that\n",
        "* Hamming distance is a metric for comparing two binary data strings.\n",
        "* While comparing two binary strings of equal length, Hamming distance is the number of bit positions in which the two bits are different.\n",
        "* The Hamming distance between two strings, a and b is denoted as d(a,b).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqa2CHp4qZIk"
      },
      "source": [
        "# Cosine similarity\n",
        "*  Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space.\n",
        "*  It is defined to equal the cosine of the angle between them, which is also the same as the inner product of the same vectors normalized to both have length 1. \n",
        "# Cosine DIstance\n",
        "*  It is measure of angular distance between two points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8delpjqrI2k"
      },
      "source": [
        "# How ro measure the effectiveness of k-NN? \n",
        "> Accuracy \n",
        "* it is effective measure of k-NN\n",
        "* Accuracy is measure as no of point for  which k-NN give corect results divided by the total test counts \n",
        "*  Accuracy=counts/n2  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJFLzNzUuPn5"
      },
      "source": [
        "# Limitation of KNN? \n",
        ">Limitation of KNN\n",
        "\n",
        "*  **Space complexity** refers to the total memory used by the algorithm. If we have n data points in training and each point is of m dimension. Then time complexity is of order O(nm), which will be huge if we have higher dimension data. Therefore, KNN is not suitable for** high dimensional data**.\n",
        "*  Does not work well with high dimensionality as this will complicate the distance calculating process to calculate distance for each dimension\n",
        "*  Sensitive to noisy and missing data\n",
        "# Advantages:\n",
        "* Easy Implementation- KNN is very easy to implement as the only thing to be calculated is the distance between different points on the basis of data of different features and this distance can easily be calculated using distance formula such as- Euclidian or Manhattan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8ZibcWw2prY"
      },
      "source": [
        "# Decision Surface for K-NN\n",
        "* K-nearest neighbor is an algorithm based on the local geometry of the distribution of the data on the feature hyperplane (and their relative distance measures). The decision boundary, therefore, comes up as nonlinear and non-smooth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI4kYUB_3HA8"
      },
      "source": [
        "# How to handdle Overfitting and Underfitting\n",
        "* Overfitting-: when k=1 , then when the decision curve is non-smooth curve , the case arriese is overfitting, since noises are taken consider and classified\n",
        "* Underfitting- when k=15, the case when the decision curve is almost linear , not taking all the point  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vffwsTSP4YtB"
      },
      "source": [
        "# Cross Validation in K-NN\n",
        "* In order to better understand the need for cross-validation, let me first recap on how to determine the right value of “K”?\n",
        "* Suppose, we are given a dataset and we split it into training data and test data in the ratio 70:30. Meaning, I keep 70% of my total data to train my model and rest 30% to test it. Next, I train my model with different values of “K” and capture its accuracy on my test data.\n",
        "* In cross-validation, instead of splitting the data into two parts, we split it into 3. Training data, cross-validation data, and test data. \n",
        "* Here, we use training data for finding nearest neighbors, we use cross-validation data to find the best value of “K” and \n",
        "* finally we test our model on totally unseen test data. This test data is equivalent to the future unseen data points.\n",
        "* Under the cross-validation part, we use D_Train and D_CV to find KNN but we don’t touch D_Test. \n",
        "* Once we find an appropriate value of “K” then we use that K-value on D_Test, which also acts as a future unseen data, to find how accurately the model performs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6e_rBqy50B2"
      },
      "source": [
        "# K-Fold cross validation ?\n",
        "* Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample\n",
        "* The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into\n",
        "* The procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation\n",
        "* Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data.\n",
        "* It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.\n",
        "> The general procedure is as follows:\n",
        "1 .Shuffle the dataset randomly\n",
        "2. Split the dataset into k groups\n",
        "3. For each unique group\n",
        "   * Take the group as a hold out or test data set\n",
        "   * Take the remaining groups as a training data set\n",
        "   * Fit a model on the training set and evaluate it on the test set\n",
        "   * Retain the evaluation score and discard the model\n",
        "4. Summarize the skill of the model using the sample of model evaluation scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xblICwd8I37"
      },
      "source": [
        "# Time based Splitting\n",
        "* In a Machine Learning algorithm we can split the given dataset into training and test data. We can either split randomly or use time based splitting.\n",
        "* For time based splitting we need a timestamp as one of the attributes / features.\n",
        "> To do this we can first sort the reviews using timestamp and then do the split.\n",
        "1. Sort data by time.\n",
        "2. Split – Training (80%) and Testing(20%)\n",
        "3. This approach can give better accuracy. Since the testing data will be more recent and hence better prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTv62ByS8fN7"
      },
      "source": [
        "# Explain k-NN for regression ?\n",
        "Same as classification problem .\n",
        "* x is input \n",
        "* y is output , y ranges any real value "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ZgTRmg9mZw"
      },
      "source": [
        "# Weighted K-NN\n",
        "* One of the many issues that affect the performance of the kNN algorithm is the choice of the hyperparameter k.\n",
        "* If k is too small, the algorithm would be more sensitive to outliers. If k is too large, then the neighborhood may include too many points from other classes.\n",
        "*> Different type wieghted function\n",
        "* >‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
        "\n",
        "* >‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
        "\n",
        "* > [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK5cuARIfzV4"
      },
      "source": [
        "   # "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}